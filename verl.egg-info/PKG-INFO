Metadata-Version: 2.4
Name: verl
Version: 0.5.0.dev0
Summary: verl: Volcano Engine Reinforcement Learning for LLM
Home-page: https://github.com/volcengine/verl
Author: Bytedance - Seed - MLSys
Author-email: zhangchi.usc1992@bytedance.com, gmsheng@connect.hku.hk
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: accelerate
Requires-Dist: codetiming
Requires-Dist: datasets
Requires-Dist: dill
Requires-Dist: hydra-core
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: peft
Requires-Dist: pyarrow>=19.0.0
Requires-Dist: pybind11
Requires-Dist: pylatexenc
Requires-Dist: ray[default]>=2.41.0
Requires-Dist: torchdata
Requires-Dist: tensordict<=0.6.2
Requires-Dist: transformers
Requires-Dist: wandb
Requires-Dist: packaging>=20.0
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: pre-commit; extra == "test"
Requires-Dist: py-spy; extra == "test"
Provides-Extra: prime
Requires-Dist: pyext; extra == "prime"
Provides-Extra: geo
Requires-Dist: mathruler; extra == "geo"
Provides-Extra: gpu
Requires-Dist: liger-kernel; extra == "gpu"
Requires-Dist: flash-attn; extra == "gpu"
Provides-Extra: math
Requires-Dist: math-verify; extra == "math"
Provides-Extra: vllm
Requires-Dist: tensordict<=0.6.2; extra == "vllm"
Requires-Dist: vllm<=0.8.5; extra == "vllm"
Provides-Extra: sglang
Requires-Dist: tensordict<=0.6.2; extra == "sglang"
Requires-Dist: sglang[openai,srt]==0.4.6.post5; extra == "sglang"
Requires-Dist: torch-memory-saver>=0.0.5; extra == "sglang"
Requires-Dist: torch==2.6.0; extra == "sglang"
Dynamic: author
Dynamic: author-email
Dynamic: home-page
Dynamic: provides-extra
Dynamic: requires-dist

# CODERL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment

This repository contains the source code, datasets, and models for the paper **"CODERL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment"**.

The CODERL+ trained models are available for download [here](https://huggingface.co/xueniki/Qwen2.5-Coder-7B-Instruct-CodeRLPLUS/tree/main).

## Training Your Own Model

If you want to train your own model, follow these steps:

### 1. Download Training Data
Download the training dataset [here](https://huggingface.co/datasets/xueniki/data_CodeRLPLUS/tree/main). The training data is derived from the [PRIME](https://arxiv.org/abs/2502.01456) work, with adjusted instructions for code RL training.

### 2. Run Training Script
Execute the training script with the following command:
```bash
bash recipe/coderlplus.sh
```
**Note:** Make sure to modify all `[]` placeholders in the script with your specific paths and configurations.

### 3. Evaluate Your Model
To evaluate the trained model, use:
```bash
bash benchmark_evaluation/eval/run.sh
```

For detailed evaluation instructions, please refer to the [evaluation guide](benchmark_evaluation/README.md).

## üìù Citation

If you find this work helpful, please cite our paper:
```bibtex
@article{jiang2025coderl+,
  title={CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment},
  author={Jiang, Xue and Dong, Yihong and Liu, Mengyang and Deng, Hongyi and Wang, Tian and Tao, Yongding and Cao, Rongyu and Li, Binhua and Jin, Zhi and Jiao, Wenpin and others},
  journal={arXiv preprint arXiv:2510.18471},
  year={2025}
}
```
